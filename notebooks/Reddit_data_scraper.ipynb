{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reddit_data_scraper.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDV_pegVY2M2",
        "colab_type": "text"
      },
      "source": [
        "# **Data collection**\n",
        "\n",
        "In order to collect the data necessary to classify posts, I explored the reddit praw API and Google BigQuery's RedditPosts dataset. The former was lacking the ability to filter posts by flair. The latter was a great option which I had considered. However, it contained posts from December 2015 to December 2019. Hence, it lacked the posts tagged as 'Coronavirus' in r/india. As a result, I decided to utilize the pushshift.io API to aquire data the latest posts from r/india. I considered a total of six flairs as per my understanding of popular posts and flairs at the current time on reddit. The following flairs were considered:\n",
        "\n",
        "```\n",
        "1.   Coronavirus\n",
        "2.   Science/Technology\n",
        "3.   Politics\n",
        "4.   Non-Political\n",
        "5.   AskIndia\n",
        "6.   Policy/Economy\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJ5Gq1lBBlej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import calendar\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmA38N4Cfu-R",
        "colab_type": "text"
      },
      "source": [
        "#### **Total posts per flair**\n",
        "\n",
        "In order to prevent class imbalance, each flair will have a maximum of 6000 posts. This number was decided based on the flair with the minimum number of posts, in this case it was Coronavirus with around 6200 posts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hbxKYSZ6XFQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "POSTS_PER_CLASS = 6000\n",
        "flairs = ['Politics', 'Coronavirus', 'Non-Political', 'Policy/Economy', 'AskIndia', 'Science/Technology']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ut9mkm2VnC6N",
        "colab_type": "text"
      },
      "source": [
        "#### **Collecting the data**\n",
        "\n",
        "The pushshift API allows the user to download over 1000 posts from a subreddit. In addition to this, it allows one to filter posts based on the date they were posted. I ran a loop which collected posts starting from current time till the total number of posts collected were greater than or equal to 6000. In the end, 6000 posts were sampled from all the posts collected from each flair."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aUm8RCGL9RX",
        "colab_type": "code",
        "outputId": "75332177-a2ad-47b2-b46c-6ffab475df63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "dataset_df = pd.DataFrame(columns=['created_utc', 'url', 'num_comments', 'selftext', 'title', 'over_18', 'link_flair_text', 'id', 'permalink'])\n",
        "\n",
        "for flair in flairs:\n",
        "  print(\"Getting data for\", flair)\n",
        "  epoch_time = calendar.timegm(time.gmtime())\n",
        "  idle_attempts = 0\n",
        "  posts = 0\n",
        "  data_dict = {'created_utc': [], 'url': [], 'num_comments': [], 'selftext': [], 'title': [], 'over_18': [], 'link_flair_text': [], \n",
        "               'id': [], 'permalink': []}\n",
        "  while posts < POSTS_PER_CLASS:\n",
        "    flag = False\n",
        "    URL = 'https://api.pushshift.io/reddit/submission/search/'\n",
        "    PARAMS = {'subreddit': 'india', 'before': epoch_time, 'sort': 'desc', 'limit': 1000} \n",
        "    r = requests.get(url = URL, params = PARAMS)\n",
        "    ret = r.json()\n",
        "    if 'data' in ret and len(ret['data']) > 0:\n",
        "      data = ret['data']\n",
        "      for ele in data:\n",
        "        if 'link_flair_text' in ele and ele['link_flair_text'] == flair:\n",
        "          flag = True\n",
        "          if ele['id'] not in data_dict['id']:\n",
        "            data_dict['created_utc'].append(ele['created_utc'])\n",
        "            data_dict['url'].append(ele['url'])\n",
        "            data_dict['num_comments'].append(ele['num_comments'])\n",
        "            data_dict['selftext'].append(ele['selftext'] if 'selftext' in ele else \"\")\n",
        "            data_dict['title'].append(ele['title'])\n",
        "            data_dict['over_18'].append(ele['over_18'])\n",
        "            data_dict['link_flair_text'].append(ele['link_flair_text'])\n",
        "            data_dict['id'].append(ele['id'])\n",
        "            data_dict['permalink'].append(ele['permalink'])\n",
        "            posts += 1\n",
        "            if posts%1000 == 0:\n",
        "              print(\"Posts:\", posts)\n",
        "      if flag is False:\n",
        "        idle_attempts += 1\n",
        "        print(\"idle attempts:\", idle_attempts)\n",
        "      if idle_attempts > 5:\n",
        "        print(\"5 idle attempts, breaking loop\")\n",
        "        break\n",
        "      epoch_time = data[len(data)-1]['created_utc']\n",
        "    else:\n",
        "      break\n",
        "  dict_df = pd.DataFrame.from_dict(data_dict)\n",
        "  print(\"Data for flair: \" + flair + \" \" + str(len(dict_df)))\n",
        "  if len(dict_df) > POSTS_PER_CLASS:\n",
        "    dict_df = dict_df.sample(n = POSTS_PER_CLASS)\n",
        "  dataset_df = pd.concat([dataset_df, dict_df], ignore_index=True)\n",
        "  print(\"Data for flair after sampling: \" + flair + \" \" + str(len(dataset_df)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Getting data for Politics\n",
            "Posts: 1000\n",
            "Posts: 2000\n",
            "Posts: 3000\n",
            "Posts: 4000\n",
            "Posts: 5000\n",
            "Posts: 6000\n",
            "Data for flair: Politics 6216\n",
            "Data for flair after sampling: Politics 6000\n",
            "Getting data for Coronavirus\n",
            "Posts: 1000\n",
            "Posts: 2000\n",
            "Posts: 3000\n",
            "Posts: 4000\n",
            "Posts: 5000\n",
            "Posts: 6000\n",
            "Data for flair: Coronavirus 6020\n",
            "Data for flair after sampling: Coronavirus 12000\n",
            "Getting data for Non-Political\n",
            "Posts: 1000\n",
            "Posts: 2000\n",
            "Posts: 3000\n",
            "Posts: 4000\n",
            "Posts: 5000\n",
            "Posts: 6000\n",
            "Data for flair: Non-Political 6048\n",
            "Data for flair after sampling: Non-Political 18000\n",
            "Getting data for Policy/Economy\n",
            "Posts: 1000\n",
            "Posts: 2000\n",
            "Posts: 3000\n",
            "Posts: 4000\n",
            "Posts: 5000\n",
            "Posts: 6000\n",
            "Data for flair: Policy/Economy 6014\n",
            "Data for flair after sampling: Policy/Economy 24000\n",
            "Getting data for AskIndia\n",
            "Posts: 1000\n",
            "Posts: 2000\n",
            "Posts: 3000\n",
            "Posts: 4000\n",
            "Posts: 5000\n",
            "Posts: 6000\n",
            "Data for flair: AskIndia 6059\n",
            "Data for flair after sampling: AskIndia 30000\n",
            "Getting data for Science/Technology\n",
            "Posts: 1000\n",
            "Posts: 2000\n",
            "Posts: 3000\n",
            "Posts: 4000\n",
            "Posts: 5000\n",
            "Posts: 6000\n",
            "Data for flair: Science/Technology 6012\n",
            "Data for flair after sampling: Science/Technology 36000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pODRrT5kax95",
        "colab_type": "code",
        "outputId": "7d9a42cc-1c45-4648-9b43-8a32182fa94d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 725
        }
      },
      "source": [
        "dataset_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>created_utc</th>\n",
              "      <th>url</th>\n",
              "      <th>num_comments</th>\n",
              "      <th>selftext</th>\n",
              "      <th>title</th>\n",
              "      <th>over_18</th>\n",
              "      <th>link_flair_text</th>\n",
              "      <th>id</th>\n",
              "      <th>permalink</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1582112360</td>\n",
              "      <td>https://v.redd.it/djadxew3avh41</td>\n",
              "      <td>23</td>\n",
              "      <td></td>\n",
              "      <td>Standing ovation and a huge round of applause ...</td>\n",
              "      <td>False</td>\n",
              "      <td>Politics</td>\n",
              "      <td>f69phh</td>\n",
              "      <td>/r/india/comments/f69phh/standing_ovation_and_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1583239508</td>\n",
              "      <td>https://www.youtube.com/watch?v=_EIDILUGKaQ</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>Narendra Modi Giving Up Social Media | Delhi s...</td>\n",
              "      <td>False</td>\n",
              "      <td>Politics</td>\n",
              "      <td>fcu1ck</td>\n",
              "      <td>/r/india/comments/fcu1ck/narendra_modi_giving_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1583032675</td>\n",
              "      <td>https://www.reddit.com/r/india/comments/fbo27x...</td>\n",
              "      <td>398</td>\n",
              "      <td>Those fuckers killing, maiming or hurting othe...</td>\n",
              "      <td>I am Hindu.</td>\n",
              "      <td>False</td>\n",
              "      <td>Politics</td>\n",
              "      <td>fbo27x</td>\n",
              "      <td>/r/india/comments/fbo27x/i_am_hindu/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1579961368</td>\n",
              "      <td>https://www.instagram.com/tv/B7tEQtmgZC0/?igsh...</td>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "      <td>Indian Prime Minister's advice makes fair and ...</td>\n",
              "      <td>False</td>\n",
              "      <td>Politics</td>\n",
              "      <td>etr6v9</td>\n",
              "      <td>/r/india/comments/etr6v9/indian_prime_minister...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1581027603</td>\n",
              "      <td>https://i.redd.it/4cgfvu5iodf41.png</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>I know it is a bit too harsh, but is BJP causi...</td>\n",
              "      <td>False</td>\n",
              "      <td>Politics</td>\n",
              "      <td>f008vi</td>\n",
              "      <td>/r/india/comments/f008vi/i_know_it_is_a_bit_to...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  created_utc  ...                                          permalink\n",
              "0  1582112360  ...  /r/india/comments/f69phh/standing_ovation_and_...\n",
              "1  1583239508  ...  /r/india/comments/fcu1ck/narendra_modi_giving_...\n",
              "2  1583032675  ...               /r/india/comments/fbo27x/i_am_hindu/\n",
              "3  1579961368  ...  /r/india/comments/etr6v9/indian_prime_minister...\n",
              "4  1581027603  ...  /r/india/comments/f008vi/i_know_it_is_a_bit_to...\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDFJOngrrteD",
        "colab_type": "text"
      },
      "source": [
        "The posts were collected and saved as a csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wzy1iiBiKYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_df.to_csv('/content/drive/My Drive/Reddit_dataset/dataset_final.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}